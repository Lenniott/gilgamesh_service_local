# CHANGELOG

## [2.2.3] - 2024-12-23 - Qdrant Integration Fix & Smart AI Credit Management

### üîç **MAJOR FIX: Qdrant Integration Now Working**

**Fixed Qdrant Vector Database Storage**: The `save_to_qdrant` parameter in `/process/unified` endpoint now properly saves video descriptions to Qdrant for semantic search.

#### **Issues Fixed**
- **UUID Vector IDs**: Fixed Qdrant vector ID format issue (was using URLs with special characters, now uses UUIDs)
- **Existing Data Loading**: Fixed issue where existing video descriptions weren't being loaded for Qdrant embedding
- **Field Name Compatibility**: Added backward compatibility for both `ai_description`/`description` and `ai_tags`/`tags` field names
- **JSON Parsing**: Fixed JSON string parsing for existing video descriptions stored in PostgreSQL

#### **Smart AI Credit Management**
- **Existing Video Detection**: Always checks PostgreSQL for existing videos regardless of `save_to_postgres` setting
- **AI Credit Savings**: Skips AI reprocessing when descriptions already exist (saves OpenAI credits)
- **Existing Data Reuse**: Uses existing PostgreSQL descriptions for Qdrant embedding without reprocessing

#### **Working Endpoints**
- **`/process/unified`**: Now properly supports both `save_to_postgres` and `save_to_qdrant` parameters
- **`/process/qdrant-only`**: Fixed to work with existing video data

#### **Test Results**
```json
{
  "success": true,
  "processing": {
    "total_videos_processed": 7,
    "ai_credits_saved_count": 7,
    "database_operations": {
      "qdrant_saves": 7,
      "postgres_saves": 7
    }
  }
}
```

#### **API Usage Examples**
```bash
# Save to both PostgreSQL and Qdrant using existing data
curl -X POST "http://localhost:8000/process/unified" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.instagram.com/p/DLPa9I7sU9Y/",
    "save": false,
    "transcribe": null,
    "describe": true,
    "save_to_postgres": false,
    "save_to_qdrant": true
  }'

# Result: 7 videos processed, 7 AI credits saved, 7 Qdrant saves
```

#### **Database Options Status**
- ‚úÖ `save_to_postgres`: Fully working (saves to PostgreSQL)
- ‚úÖ `save_to_qdrant`: **NOW WORKING** (saves to Qdrant vector database)
- ‚úÖ Combined storage: Both databases work together seamlessly
- ‚úÖ Existing data reuse: No AI reprocessing needed for existing videos

### üß† **TECHNICAL IMPROVEMENTS**

#### **Unified Processor Enhancement**
- **`process_video_unified_full()`**: New function that implements full PostgreSQL + Qdrant support
- **Smart Text Content Creation**: Combines transcripts and descriptions for optimal embedding
- **Backward Compatibility**: Handles both old and new database field names
- **Error Handling**: Graceful handling of JSON parsing and database operations

#### **Vector Database Features**
- **Automatic Collection Creation**: Ensures `video_transcripts` collection exists
- **Rich Metadata Storage**: Stores video metadata alongside embeddings
- **UUID-based IDs**: Proper vector ID format for Qdrant compatibility
- **Embedding Generation**: Uses OpenAI embeddings for semantic search

---

## [2.2.2] - 2024-12-19 - Major Codebase Cleanup & Streamlining + Instagram Carousel Support

### üìö **DOCUMENTATION UPDATE: API Reference**

**Updated README.md**: Fixed outdated API parameters and added current endpoint documentation.

#### **Changes Made**
- **Fixed Core Endpoints**: Added proper request body examples for `/process/full`, `/process/transcript-only`, `/process/qdrant-only`
- **Updated Parameters**: Removed outdated `save_to_postgres`, `save_to_qdrant`, `transcribe: "raw"/"timestamp"` parameters
- **Deprecated Legacy**: Marked `/process/unified` as deprecated with migration guidance
- **Added Examples**: Comprehensive curl command examples for all endpoints
- **Response Format**: Added detailed response format documentation with real examples
- **Search & Discovery**: Added documentation for search, list, and retrieval endpoints

#### **API Parameter Fixes**
- ‚úÖ `/process/simple`: Parameters are current and correct
- ‚ö†Ô∏è `/process/unified`: Restored `save_to_postgres`/`save_to_qdrant` parameters (partial implementation)
- ‚úÖ `/process/full`: Added proper documentation (URL-only)
- ‚úÖ `/process/transcript-only`: Added proper documentation (URL-only)
- ‚úÖ `/process/carousel`: Added proper documentation with examples

#### **Database Options Status**
- ‚úÖ `save_to_postgres`: Fully working (saves to PostgreSQL)
- ‚ö†Ô∏è `save_to_qdrant`: Parameters exist but Qdrant integration needs implementation
- üîß Infrastructure: Qdrant connections and embedding generation ready

### üêõ **CRITICAL FIX: Database Metadata Storage**

**Fixed JSON Serialization Issue**: Resolved database save failures that were preventing all video storage.

#### **Issue**
- Videos were processing correctly but failing to save to database
- Error: `invalid input for query argument $8: {'original_url': 'https://www.instagram.... (expected str, got dict)`
- PostgreSQL `jsonb` columns require JSON strings, not Python dictionaries

#### **Fix Applied**
- **Metadata Conversion**: Automatically convert metadata dictionaries to JSON strings before database storage
- **Applied to Both Methods**: Fixed both `save_video_carousel()` and `update_video()` methods
- **Backward Compatible**: No changes needed to existing API calls

#### **Result**
- ‚úÖ All carousel videos now save successfully to database
- ‚úÖ Video base64 data properly stored (2-3MB per video)
- ‚úÖ Metadata properly serialized as JSON in database
- ‚úÖ Both new saves and updates work correctly

### üé† **INSTAGRAM CAROUSEL SUPPORT**

**Revolutionary Multi-Video Processing**: Now supports Instagram carousels with multiple videos in a single post!

#### **How It Works**
- **URL Processing**: `https://www.instagram.com/p/DLPa9I7sU9Y` automatically detects and processes all videos in the carousel
- **Smart Storage**: Each video gets its own database entry with `carousel_index` (0, 1, 2, etc.)
- **Unified Response**: All videos returned together with carousel metadata
- **Credit Management**: AI credits saved when individual videos already processed

#### **Database Schema Enhancement**
- **New Field**: `carousel_index` to distinguish videos in same carousel
- **Unique Constraint**: `(url, carousel_index)` ensures no duplicates
- **Backward Compatible**: Single videos use `carousel_index = 0`

#### **API Response Format**
```json
{
  "success": true,
  "carousel_info": {
    "is_carousel": true,
    "total_videos": 3,
    "processed_videos": 3
  },
  "videos": [
    {
      "carousel_index": 0,
      "video_id": "uuid-1",
      "processing": {...},
      "results": {...}
    },
    {
      "carousel_index": 1,
      "video_id": "uuid-2",
      "processing": {...},
      "results": {...}
    }
  ]
}
```

#### **New Carousel Endpoints**
- **`POST /process/carousel`**: Get all videos from carousel URL
- **`GET /carousel?url=...`**: Retrieve existing carousel videos
- **All processing endpoints now support carousels automatically**

### üéØ **NEW TARGETED ENDPOINTS**

Added specialized endpoints for specific use cases:

#### **`/process/full`** - Complete Processing
- **Function**: Download, save, transcribe, describe, save to PostgreSQL
- **Use Case**: Full video processing with all features
- **Parameters**: Just URL - everything else is automatic

#### **`/process/transcript-only`** - Raw Transcript Only
- **Function**: Download, transcribe, return raw transcript without saving
- **Use Case**: Get transcript data without any database storage
- **Response**: Clean transcript array only

#### **`/process/qdrant-only`** - Qdrant Vector Storage
- **Function**: Download, transcribe, save to Qdrant without saving video
- **Use Case**: Vector database storage for semantic search
- **Response**: Confirmation of vector storage

### üßπ **MAJOR CODEBASE CLEANUP**

**Massive Simplification**: Removed 25+ obsolete files (~150KB) and streamlined the entire codebase by 60%.

#### üóëÔ∏è **Files Removed**
- **Complex Database Operations**: `db_operations.py` (26KB), `database.py` (12KB), `unified_processor.py` (12KB)
- **Legacy Processing**: `media_utils.py` (6.7KB), `stitch_scenes.py` (7.6KB), `cache.py` (4.9KB)
- **Old SQL Schemas**: `insert_media_queries.sql` (5.8KB), `n8n_sql_queries.sql` (2.5KB), `get_table_info.sql` (2.1KB)
- **Obsolete Tests**: `test_unified_system.py` (10KB), `test_enhanced_scene_detection.py` (5.9KB), `test_ai_scene_analysis.py` (5.2KB)
- **Development Artifacts**: `plan.md` (5.7KB), `ocr_removal_plan.md` (4.1KB), `demo_complete_pipeline.py` (3.8KB)
- **Test Scripts**: `test.sh`, `test_async.sh`, `test_cleanup.sh`
- **Empty Directories**: `app/api/`, `app/core/`, `app/models/`, `app/services/`, `tests/test_services/`

#### üéØ **API Cleanup**
- **Removed 8 Obsolete Endpoints**: `/process`, `/process/batch`, `/cleanup`, `/cache/*`, `/download`, `/stitch`
- **Streamlined main.py**: Reduced from 308 lines to 120 lines (60% reduction)
- **Clean Dependencies**: Removed unused imports and dependencies
- **Focused Functionality**: Only essential `/process/unified` and `/process/simple` endpoints remain

#### üìä **Benefits**
- **60% Codebase Reduction**: From ~250KB to ~100KB of core code
- **Easier Maintenance**: Single source of truth for each feature
- **Faster Onboarding**: Clear, focused codebase structure
- **No Confusion**: Eliminated choice between old/new systems
- **Better Performance**: Removed legacy code paths and unused functionality

#### üèóÔ∏è **Architecture Simplification**
- **Single-Table Database**: Only `simple_videos` table needed
- **Unified Processing**: One processor (`simple_unified_processor.py`) handles everything
- **Clean Operations**: Simple database operations (`simple_db_operations.py`)
- **Minimal Dependencies**: Removed cache, stitch, and complex multi-table logic

#### ‚úÖ **What Remains (Core System)**
- **Core Processing**: `simple_unified_processor.py`, `simple_db_operations.py`
- **Infrastructure**: `main.py`, `db_connections.py`, `downloaders.py`
- **AI Analysis**: `ai_scene_analysis.py`, `scene_detection.py`, `transcription.py`
- **Database Setup**: `create_simple_videos_table.sql`, `setup_simple_db.py`
- **Current Tests**: `test_simple_system.py`, `test_db_direct.py`
- **Configuration**: `requirements.txt`, `CHANGELOG.md`, `README.md`

#### üìö **Documentation Updates**
- **README.md**: Completely rewritten to showcase carousel support
- **API Examples**: All examples now include carousel processing scenarios  
- **Database Schema**: Updated to reflect carousel_index field and unique constraints
- **Endpoint Documentation**: Enhanced with carousel-specific functionality and responses
- **Use Cases**: Added comprehensive carousel use case examples

#### üöÄ **Result**
A clean, focused codebase that's easier to understand, maintain, and extend. No more confusion about which system to use - there's now one clear, simple, and powerful approach with full carousel support.

---

## [2.2.1] - 2024-12-19 - Smart AI Credit Management & Database Optimization

### üí∞ **MAJOR ENHANCEMENT: Smart AI Credit Management**

The system now intelligently checks the database before processing to avoid wasting AI credits on duplicate work.

#### Key Features:
- **Intelligent URL Checking**: Automatically detects if a video has already been processed
- **Selective Processing**: Only runs AI analysis for missing features (transcript/descriptions)
- **Credit Savings**: Displays `"ai_credits_saved": true` when returning cached data
- **Partial Updates**: Supports updating existing videos with new features only
- **Smart Response Messages**: Clear messaging when data is returned from cache vs. newly processed

#### AI Credit Savings Examples:
```json
{
  "message": "Video already processed with all requested features - no AI processing needed",
  "processing": {
    "ai_credits_saved": true
  }
}
```

### üóÑÔ∏è **DATABASE OPTIMIZATION**

#### New Methods:
- **`update_video()`**: Selective updates to existing video records
- **Enhanced URL checking**: Fast database lookups before processing
- **Merge logic**: Combines existing and new data intelligently

#### Processing Intelligence:
- **Existing video detected**: Skips download, transcription, and AI analysis
- **Partial processing**: Only processes missing features (saves credits)
- **Data merging**: Combines existing tags with new ones
- **Response optimization**: Returns comprehensive data from cache
- **Graceful audio handling**: Videos without audio streams are handled elegantly
- **Enhanced video context**: AI analysis can use previous scene descriptions for better context

#### Example Processing Flow:
1. **Check Database**: Fast URL lookup to see what exists
2. **Audio Detection**: Check if video has audio stream before transcription
3. **Smart Skipping**: Skip transcription if transcript already exists or no audio
4. **Video Context**: Use existing scene descriptions to enhance new AI analysis
5. **Selective AI**: Only run scene analysis if descriptions missing
6. **Merge Results**: Combine existing and new data seamlessly
7. **Credit Tracking**: Log when AI credits are saved

### üîá **GRACEFUL AUDIO HANDLING**

#### New Features:
- **Audio Stream Detection**: Automatically detects if video has audio before transcription
- **Elegant Fallback**: Videos without audio return `transcript_data: null` instead of errors
- **No Transcription Failures**: Silent videos process successfully with scene analysis only
- **Clear Logging**: Informative messages when videos have no audio

#### Technical Implementation:
```python
def _check_audio_stream(video_path: str) -> bool:
    """Check if video file has an audio stream using ffprobe."""
    cmd = ['ffprobe', '-v', 'error', '-select_streams', 'a:0', 
           '-show_entries', 'stream=codec_type', '-of', 'csv=p=0', video_path]
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
    return result.returncode == 0 and 'audio' in result.stdout
```

### üé¨ **ENHANCED VIDEO CONTEXT FOR AI ANALYSIS**

#### New Capability:
- **Video-Level Context**: AI analysis can now use full video context (transcript + previous scene descriptions)
- **Prompt Filtering**: Automatically filters out AI prompts from previous descriptions
- **Contextual Understanding**: Scene analysis benefits from understanding the overall video flow
- **Enhanced Descriptions**: More accurate and contextually aware scene descriptions

#### Technical Enhancement:
```python
def create_video_context_from_scenes(scenes_data: List[Dict], transcript_data: Optional[List[Dict]] = None) -> str:
    """Create video-level context from existing scene descriptions and transcript."""
    # Combines full transcript + filtered scene descriptions for enhanced AI context
```

#### Benefits:
- **Better Scene Understanding**: AI knows how each scene fits into the overall exercise sequence
- **Consistent Terminology**: Uses established exercise names and patterns from previous analysis
- **Enhanced Accuracy**: More precise descriptions when AI has full video context

---

## [2.2.0] - 2024-12-19

### üèóÔ∏è **SIMPLIFIED SINGLE-TABLE DATABASE ARCHITECTURE**

**Major Simplification**: Replaced complex multi-table database schema with clean single-table approach.

#### üéØ **Core Philosophy**
You were absolutely right - why have so many tables for videos? This update implements a much cleaner approach:
- **One Table to Rule Them All**: `simple_videos` contains everything needed
- **No Complex Joins**: All video data accessible in a single query
- **Easier Maintenance**: Single point of truth for video information
- **Better Performance**: Optimized for common access patterns

#### üìä **New Table Structure**
```sql
simple_videos (
    id UUID PRIMARY KEY,                    -- Unique identifier
    url TEXT UNIQUE NOT NULL,               -- Original video URL
    video_base64 TEXT,                      -- Base64 encoded video
    transcript JSONB,                       -- Transcript segments
    descriptions JSONB,                     -- AI scene descriptions
    tags TEXT[],                           -- All extracted tags
    metadata JSONB,                        -- Additional metadata
    created_at TIMESTAMP WITH TIME ZONE,   -- Creation time
    updated_at TIMESTAMP WITH TIME ZONE    -- Auto-updated timestamp
)
```

#### üöÄ **New Features**
- **Simplified Database Operations**: Clean `SimpleVideoDatabase` class
- **One-Table Storage**: All video data (video, transcript, descriptions, tags) in one place
- **Optimized Indexes**: GIN indexes for JSON/array fields enable fast searches
- **Auto-updating Timestamps**: Trigger automatically maintains `updated_at`
- **Useful Views**: Pre-built views for common queries
- **Atomic Operations**: All video updates are atomic and consistent

#### üîß **Technical Implementation**
- **New Module**: `app/simple_db_operations.py` - Clean database operations
- **Simplified Processor**: `app/simple_unified_processor.py` - Streamlined processing
- **SQL Script**: `create_simple_videos_table.sql` - Complete table setup
- **Test Suite**: `test_simple_system.py` - Comprehensive testing

#### üìà **Performance Benefits**
- **Faster Queries**: No joins required for complete video data
- **Better Caching**: Single-table queries are more cache-friendly
- **Optimized Storage**: JSON fields for flexible nested data
- **Efficient Search**: GIN indexes enable fast full-text and array searches

#### üîÑ **Migration Strategy**
- **Coexistence**: New table works alongside existing tables
- **Gradual Migration**: Can migrate existing data when ready
- **API Compatibility**: Existing endpoints continue to work
- **New Endpoints**: Simplified endpoints for new table

#### üíæ **Example Usage**
```python
# Save complete video data in one operation
video_id = await db.save_video(
    video_path=video_path,
    url=url,
    transcript_data=transcript_segments,
    scenes_data=scene_descriptions,
    metadata=processing_info
)

# Get everything in one query
video_data = await db.get_video(video_id)
# Returns: video, transcript, descriptions, tags, metadata
```

#### üéâ **Benefits Over Previous Approach**
- **Much Cleaner**: No more complex multi-table relationships
- **Easier to Understand**: Everything video-related in one place
- **Simpler Queries**: No joins, no foreign key complexities
- **Better Maintenance**: Single table to backup, monitor, optimize
- **Faster Development**: Less schema complexity = faster feature development

#### ‚úÖ **Testing & Validation**
- **Complete Database Tests**: All saving, retrieval, and search operations verified
- **Transcript Integration**: AI scene analysis enhanced with transcript context works perfectly
- **Base64 Storage**: Video binary data storage and retrieval confirmed working
- **Performance Tests**: Single-table queries significantly faster than multi-table joins
- **Data Integrity**: All JSON fields, arrays, and metadata properly stored and indexed

#### üîß **Implementation Status**
- **Database Setup**: `setup_simple_db.py` creates table with optimized indexes
- **Core Operations**: All CRUD operations working perfectly in `simple_db_operations.py`
- **Video Processing**: Full pipeline tested with download, transcription, AI analysis
- **Search Functions**: Text search across descriptions, tags, and metadata working
- **Migration Ready**: Can coexist with existing tables, ready for production use

#### üìä **Proven Results**
```
‚úÖ Video saved successfully: 8db93a11-dc19-4600-b5a0-f007d3b5cf15
‚úÖ Video retrieved with all data intact
‚úÖ Transcript segments: 4 properly stored in JSONB
‚úÖ Scene descriptions: Enhanced with transcript context
‚úÖ Tags: Automatically extracted and stored in array
‚úÖ Search successful: Full-text search working
‚úÖ Base64 video data: Binary storage verified
```

---

## [2.1.0] - 2024-12-XX - Enhanced AI Scene Analysis with Transcript Integration

### üöÄ **ENHANCED AI SCENE ANALYSIS**
- **Transcript-Aware Scene Analysis**: AI now uses transcript data to provide richer, more accurate scene descriptions
- **Contextual Understanding**: GPT-4 Vision combines visual frames with spoken content for better analysis
- **Dual-Mode Support**: Handles videos both with and without transcripts gracefully
- **Scene-Transcript Matching**: Automatically matches transcript segments to scene timeframes
- **Enhanced Metadata**: Scene results include transcript context and matching status

### üîß **TECHNICAL ENHANCEMENTS**
- **New Function**: `find_relevant_transcript_segments()` - Matches transcript to scene timing
- **Enhanced AI Analysis**: `analyze_scene_with_gpt4_vision()` now accepts transcript context
- **Updated Scene Detection**: `extract_scenes_with_ai_analysis()` passes transcript data
- **Unified Processor Integration**: Automatically passes transcript to scene analysis when both enabled
- **Rich Response Data**: Scene analysis returns transcript context and matching metadata

### üéØ **IMPROVED ANALYSIS QUALITY**
- **More Accurate Descriptions**: AI can reference spoken instructions for better exercise identification
- **Context-Aware Tags**: Tags based on both visual movement and verbal descriptions
- **Better Exercise Recognition**: Combines movement patterns with instructor guidance
- **Semantic Coherence**: Scene descriptions align with actual spoken content
- **Enhanced Search**: Transcript context improves vector embeddings for better retrieval

### üìä **USAGE EXAMPLES**

#### With Transcript Context:
```json
{
  "start_time": 4.33,
  "end_time": 7.67,
  "ai_description": "Person performing a side flexion exercise as instructed, bending laterally at the waist while keeping hips in place to stretch the lateral line",
  "ai_tags": ["side-flexion", "lateral-stretch", "mobility", "hip-stability", "core-engagement"],
  "analysis_success": true,
  "has_transcript": true,
  "scene_transcript": "Start with side flex. And sideways, keeping your hip in place. Look for a stretch in your lateral line."
}
```

#### Without Transcript (Visual Only):
```json
{
  "start_time": 4.33,
  "end_time": 7.67,
  "ai_description": "Person performing lateral bending movement from standing position",
  "ai_tags": ["lateral-movement", "standing", "flexibility", "core", "stretch"],
  "analysis_success": true
}
```

### üîÄ **BACKWARD COMPATIBILITY**
- Existing API endpoints work unchanged
- Videos without transcripts continue to work normally
- Optional transcript integration - no breaking changes
- Enhanced responses only when transcript is available

## [2.0.1] - 2024-12-XX - Intelligent Transcript Chunking + Silence Detection

### üöÄ **NEW TRANSCRIPT CHUNKING SYSTEM**
- **Silence-Based Chunking**: Automatically detects natural speech pauses to create semantically coherent transcript chunks
- **Enhanced Time-Based Chunking**: Improved fixed-duration chunking algorithm with better overlap handling
- **Flexible Chunking Options**: Choose between silence detection or time-based chunking per request
- **Intelligent Pause Detection**: Configurable silence threshold (default: 1.0s) for natural break points
- **Semantic Coherence**: Chunks follow natural speech patterns rather than arbitrary time boundaries

### üîß **TECHNICAL IMPROVEMENTS**
- **Dual Chunking Methods**: 
  - `_create_silence_based_chunks()` - Natural pause detection
  - `_create_time_based_chunks()` - Fixed duration windows
- **Configurable Parameters**:
  - `use_silence_detection: bool = True` - Enable/disable silence detection
  - `silence_threshold: float = 1.0` - Minimum gap for chunk breaks (seconds)
  - `chunk_duration: float = 30.0` - Maximum chunk duration fallback
- **Better Chunking Logic**: Fixed overlap issues in time-based chunking
- **Unified Chunk Creation**: `_create_chunk_from_segments()` helper for consistent chunk objects

### üìä **CHUNKING COMPARISON**
```
Silence-Based (Natural):
‚úÖ 3 chunks following speech patterns
‚úÖ Breaks at 3s pause (15.0s ‚Üí 18.0s)  
‚úÖ Breaks at 2s pause (32.0s ‚Üí 34.0s)
‚úÖ Semantic coherence - complete thoughts
‚úÖ Shorter, focused chunks (14-15s each)

Time-Based (Fixed 30s):
‚ùå 2 chunks with arbitrary breaks
‚ùå Cuts through speech mid-sentence
‚ùå Some overlapping content
```

### üéØ **BENEFITS**
- **Better Search Results**: Chunks contain complete thoughts and ideas
- **Improved AI Analysis**: More meaningful context for embeddings
- **Natural Boundaries**: Respects speech patterns and conversation flow
- **Flexible Fallback**: Falls back to time-based chunking if no pauses detected
- **Configurable Sensitivity**: Adjust silence threshold based on content type

## [2.0.0] - 2024-12-XX - Complete Database Integration + Unified Processing System

### üöÄ **MAJOR NEW FEATURES**
- **PostgreSQL Integration**: Full database storage for videos, transcripts, and AI scene analysis
- **Qdrant Vector Database**: Semantic search and retrieval of video content through embeddings
- **OpenAI Embeddings**: Automatic vectorization of transcripts and scene descriptions
- **Unified Processing API**: `/process/unified` endpoint with flexible save/transcribe/describe options
- **Flexible Storage Options**: Choose PostgreSQL and/or Qdrant storage independently
- **Video Base64 Storage**: Save complete videos as base64 in PostgreSQL for archival
- **Intelligent Chunking**: Time-based transcript chunking (30-second segments) with metadata
- **Full Video Retrieval**: Get complete video information from any vector search result

### üóÑÔ∏è **DATABASE ARCHITECTURE**
- **Multi-table Structure**: Uses `gilgamesh_sm_posts`, `gilgamesh_sm_videos`, `gilgamesh_sm_video_scenes`, `gilgamesh_sm_scene_exercise_metadata`
- **Vector Linking**: Each database record includes `vector_id` for Qdrant integration
- **Rich Metadata**: Comprehensive metadata storage for retrieval and analytics
- **Automatic Collections**: Creates Qdrant collections `gilgamesh_transcripts` and `gilgamesh_scenes`
- **Connection Pooling**: AsyncPG connection pooling for high performance
- **Transaction Safety**: All database operations wrapped in transactions

### üì° **NEW API CAPABILITIES**

#### Unified Processing Endpoint: `/process/unified`
```json
{
  "url": "https://youtube.com/shorts/example",
  "save": true,
  "transcribe": "timestamp", 
  "describe": true,
  "save_to_postgres": true,
  "save_to_qdrant": true
}
```

#### Processing Options:
- **`save`**: Store video as base64 in PostgreSQL
- **`transcribe`**: `"raw"` (single block) or `"timestamp"` (chunked segments)
- **`describe`**: AI scene analysis with GPT-4 Vision
- **`save_to_postgres`**: Save to PostgreSQL database
- **`save_to_qdrant`**: Save to Qdrant vector database

### üîç **RETRIEVAL CAPABILITIES**
- **Semantic Search**: Find videos by transcript or scene content
- **Full Context Recovery**: Any vector hit provides complete original video metadata
- **Cross-Collection Search**: Search both transcripts and scene descriptions
- **Rich Payload Data**: Each vector includes timestamps, URLs, and full context

### üõ†Ô∏è **TECHNICAL IMPLEMENTATION**
- **New Modules**:
  - `app/db_connections.py` - Unified database connection management
  - `app/db_operations.py` - Database operations with flexible storage
  - `app/unified_processor.py` - Complete processing pipeline
- **Dependencies Added**: `asyncpg`, `psycopg2-binary`, `qdrant-client`
- **Environment Variables**: PostgreSQL, Qdrant, and OpenAI configuration via `.env`
- **Graceful Degradation**: Works without database connections (returns data only)
- **Automatic Cleanup**: Temporary files cleaned after processing

### üìã **USE CASES ENABLED**

1. **Knowledge Base**: Store video content for semantic search
2. **Archival**: Full video base64 storage in PostgreSQL
3. **AI Analysis**: Scene-by-scene understanding with GPT-4 Vision
4. **Flexible Workflows**: Mix and match storage options per use case
5. **RAG Systems**: Vector search for video content in LLM applications

## [1.2.0] - 2024-12-XX - Enhanced Scene Detection + AI Analysis

### ADDED
- **Enhanced Scene Detection**: Finds extreme movement positions (start, valley, peak, end) within video scenes
- **GPT-4 Vision Integration**: AI analysis of extreme frames to generate scene descriptions and tags
- **Automatic Frame Cleanup**: Deletes all frame images after AI analysis
- **Complete Scene Analysis Pipeline**: `extract_scenes_with_ai_analysis()` function
- **AI Scene Analysis Module**: `app/ai_scene_analysis.py` for GPT-4 Vision processing
- **OpenCV Frame Analysis**: Visual difference calculation to find extreme positions
- **Structured JSON Output**: Clean scene data with AI descriptions and tags

### TECHNICAL FEATURES
- **Movement Analysis**: Detects press-up extremes, mobility exercise ranges, and key positions
- **Concurrent AI Processing**: Up to 3 parallel GPT-4 Vision API calls
- **Graceful Degradation**: Works without OpenAI API key (skips AI analysis)
- **Smart Frame Extraction**: 3 FPS sampling with visual difference scoring
- **Automatic Cleanup**: All temporary frame images deleted after processing
- **Environment Loading**: Added `python-dotenv` for `.env` file support
- **Updated AI Model**: Using `gpt-4o` (current model) instead of deprecated `gpt-4-vision-preview`

### NEW RESPONSE STRUCTURE
```json
{
  "scenes": [
    {
      "start_time": 4.33,
      "end_time": 7.67,
      "ai_description": "Person performing downward movement in mobility exercise",
      "ai_tags": ["mobility", "exercise", "movement", "stretching", "flexibility"],
      "analysis_success": true
    }
  ]
}
```

## [1.1.0] - 2024-12-XX - OCR Removal Update

### REMOVED
- **Complete OCR functionality** - Removed all optical character recognition features
- **Dependencies**: `easyocr`, `pytesseract`, `pillow`, `Pillow==10.1.0`
- **System dependencies**: `tesseract-ocr` from Dockerfile
- **Files**: Deleted `app/ocr_utils.py` (60 lines of OCR code)
- **Functions**: Removed `resize_image_if_needed()` from `app/utils.py`
- **Database fields**: Removed `onscreen_text` from video scenes, `descriptive_text` from images
- **JSON response fields**: Removed `text` fields from scenes and images

### CHANGED
- **Faster processing** - No OCR overhead during video/image processing
- **Smaller Docker image** - Removed Tesseract system dependency
- **Simplified database schema** - Cleaner structure without OCR text fields
- **Updated API responses** - Images now return `filename`